{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a74e4-bdff-4c0c-adec-58d7f643bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from langchain.chains import StuffDocumentsChain, RetrievalQA, LLMChain, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings, OllamaEmbeddings\n",
    "from langchain.llms import Ollama, BaseLLM\n",
    "from langchain.schema import Document, Generation, LLMResult\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "class LocalOllamaLLM(BaseLLM):\n",
    "    api_url : str\n",
    "    def _generate(self, prompt, stop):\n",
    "        response = requests.post(f\"{self.api_url}/api/generate\", json={\"model\": \"mistral-large\", \"prompt\": str(prompt) })\n",
    "        response.raise_for_status()\n",
    "        response_text=''.join([json.loads(line)['response'] for line in response.text.splitlines()])\n",
    "        generations=[]\n",
    "        generations.append([Generation(text=response_text)])\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "\n",
    "    def _llm_type(self):\n",
    "        return \"local\"  # Or whatever type is appropriate for your local setup\n",
    "\n",
    "llm = LocalOllamaLLM(api_url=\"http://127.0.0.1:11434\")\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "vector_store = Chroma(embedding_function=embedder, persist_directory=\"./chroma_db\")\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \" Répondez à la question posée \"\n",
    "    \" Utilisez le contexte (sélection des meilleurs paragraphes liés à la question) donné pour répondre à la question \"\n",
    "    \" Si la réponse ne se trouve pas dans le contexte, répondez par 'Je ne sais pas'\"\n",
    "    \" Contexte : {context}  \"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "def search_and_invoke_llm(vector_store,index,query,k=5):\n",
    "    if k==0:\n",
    "        print(f\"bug with {index}\")\n",
    "        return None\n",
    "    else:\n",
    "        pass\n",
    "    try:\n",
    "        retriever=vector_store.as_retriever(\n",
    "        search_kwargs={\n",
    "                \"k\": k, \n",
    "                \"filter\": {'index': index}\n",
    "            }\n",
    "        )\n",
    "        chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "        result=chain.invoke({\"input\": query})\n",
    "        return result\n",
    "    except:\n",
    "        search_and_invoke_llm(vector_store,index,query,k=k-1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c603d-1a06-4cac-9133-50671084d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS=[\"Quelles sont les règles en nombre de jours épargnés pour les CET ?\"\n",
    ",\"Qui sont les bénéficiaires du CET ?\"\n",
    ",\"Quelles sont les conditions d'égibilité du CET ?\"\n",
    ",\"Quel est le plafond annuel du CET ?\"\n",
    ",\"Quel est le plafond total max du CET ?\"\n",
    ",\"Quelles sont les conditions de déblocage des jours de congés CET ?\"\n",
    ",\"Peut on monétiser ses jours de congés CET ?\"\n",
    ",\"Quel est le délai de prévenance pour débloquer ou liquider ses jours de congés CET ?\"\n",
    ",\"Quelles sont les règles particulières pour les séniors?\"\n",
    ",\"Quelles sont les conditions de transfert du CET?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9917abc5-d9b8-4645-a098-39a8b055c836",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_YAML=\"\"\"\n",
    "beneficiaire: str/None\n",
    "plafond_annuel_max: int/None\n",
    "plafond_total_max: int/None\n",
    "monetisation: \n",
    "    regle_monetisation : bool/None\n",
    "    limite_monetisation: int/None\n",
    "    delai_prevenance_monetisation: bool/None\n",
    "anciennete_minimum: int/None\n",
    "regles_specifiques_senior: bool/None\n",
    "transfert_droits: bool/None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a676a-ae0c-4952-bca8-d542bab44f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"./10p_accords_publics_et_thematiques_240815_sub_CET.parquet\"\n",
    "df=pd.read_parquet(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38600d7-a036-4046-bd23-46234a50ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt2 = (\n",
    "    \"Formate la réponse suivante selon ce schema YAML suivant : \" +FORMAT_YAML\n",
    ")\n",
    "prompt_template2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt2),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain2 = prompt_template2 | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5106c-409a-45a8-bd06-43fb865a0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_df=[]\n",
    "Path(\"results\").mkdir(parents=True, exist_ok=True)\n",
    "for index, row in df.iterrows():\n",
    "    dict_answer=dict()\n",
    "    answer=\"\"\n",
    "    for Q0 in QUESTIONS:\n",
    "        if ans:=search_and_invoke_llm(vector_store,index,Q0,k=2):\n",
    "            answer_txt=ans['answer']\n",
    "            answer+=answer_txt\n",
    "    \n",
    "    reponse=chain2.invoke({\"input\": answer})\n",
    "    with open(f\"results/{index}.answer\",\"w\") as file:\n",
    "        file.write(reponse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
