{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f5525b-6bac-404d-9724-96de51d76185",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp s3/$VAULT_TOP_DIR/Accords/Construction_dataset_public/Dataset_public_accords_teletravail_Dares.parquet ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c8221-e6eb-4392-99a1-70d48a680d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp -r s3/$VAULT_TOP_DIR/Accords/chroma_db ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e5d88-a743-4eed-8aa6-5806e6461f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, OllamaEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document, Generation, LLMResult\n",
    "from langchain.llms import Ollama, BaseLLM\n",
    "from langchain.chains import StuffDocumentsChain, RetrievalQA, LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "\n",
    "#\"seed\":1337,\"temperature\":1,\"num_ctx\":100,\n",
    "#,\"options\":{\"stop\":[\",\",\".\"]}\n",
    "class LocalOllamaLLM(BaseLLM):\n",
    "    api_url : str\n",
    "    def _generate(self, prompt, stop):\n",
    "        response = requests.post(f\"{self.api_url}/api/generate\", json={\"model\": \"llama3.1\", \"prompt\": str(prompt) })\n",
    "        response.raise_for_status()\n",
    "        response_text=''.join([json.loads(line)['response'] for line in response.text.splitlines()])\n",
    "        generations=[]\n",
    "        generations.append([Generation(text=response_text)])\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "\n",
    "    def _llm_type(self):\n",
    "        return \"local\"  \n",
    "\n",
    "\n",
    "\n",
    "def search_and_invoke_llm(vector_store,index,query,k=5):\n",
    "    if k==0:\n",
    "        print(f\"bug with {index}\")\n",
    "        return None\n",
    "    elif k==1:\n",
    "        pass\n",
    "        #print(f\" trying k=1 : {index}\")\n",
    "    else:\n",
    "        pass\n",
    "    try:\n",
    "        retriever=vector_store.as_retriever(\n",
    "        search_kwargs={\n",
    "                \"k\": k, \n",
    "                \"filter\": {'index': index}\n",
    "            }\n",
    "        )\n",
    "        chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "        result=chain.invoke({\"input\": query})\n",
    "        return result\n",
    "    except:\n",
    "        search_and_invoke_llm(vector_store,index,query,k=k-1)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "llm = LocalOllamaLLM(api_url=\"http://127.0.0.1:11434\")\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "system_prompt = (\n",
    "    \" Répondez à la question posée \"\n",
    "    \" Utilisez le contexte (sélection des meilleurs paragraphes liés à la question) donné pour répondre à la question \"\n",
    "    \" Répondez en bullet point comme ceci : \"\n",
    "    \" ### Règles : \"\n",
    "    \" * (règle 1) \"\n",
    "    \" * (règle 2) \"\n",
    "\n",
    "    #\" Si la réponse ne se trouve pas dans le contexte, répondez par 'Non'\"\n",
    "    \" Contexte : {context}  \"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c71950-76d0-42f9-8e27-45840083a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"Dataset_public_accords_teletravail_Dares.parquet\"\n",
    "df=pd.read_parquet(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370241b6-4a06-4474-87ef-889efcf7b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(embedding_function=embedder,persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03b4133-fbe1-49e5-a493-7f8075788b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q0=\"Est-ce que les cadres télétravaillent différemment des non-cadres ?\"\n",
    "\n",
    "LIST_OF_QUESTIONS=[Q0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9eae4c-bdbc-4437-9583-fcd8cd77d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "already_done={el.split(\"/\")[1].split(\".\")[0] for el in glob(\"results/*.answer\")}\n",
    "new_dir = Path('results').mkdir(exist_ok=True)\n",
    "for index, row in df.iterrows():\n",
    "    answer=\"\"\n",
    "    if index not in already_done:\n",
    "\n",
    "        for Q in LIST_OF_QUESTIONS:\n",
    "            if ans:=search_and_invoke_llm(vector_store,index,Q,k=2):\n",
    "                answer_txt=ans['answer']\n",
    "                answer += answer_txt\n",
    "            answer += \"\\n-----\\n\"\n",
    "        print(index,df.nombre_jour_hebdo_TT_annotee[index], answer)\n",
    "        if answer:\n",
    "            with open(f\"results/{index}.answer\",\"w\") as f:\n",
    "                f.write(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47593af5-4d43-4129-8e85-6cb8245a725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp -r results s3/$VAULT_TOP_DIR/Accords/Teletravail/Runs/Llama3.1/Cadres/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe77bd-1e0b-48ac-8f5a-66005efce340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
