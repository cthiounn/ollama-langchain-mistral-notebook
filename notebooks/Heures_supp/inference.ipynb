{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a74e4-bdff-4c0c-adec-58d7f643bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from langchain.chains import StuffDocumentsChain, RetrievalQA, LLMChain, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings, OllamaEmbeddings\n",
    "from langchain.llms import Ollama, BaseLLM\n",
    "from langchain.schema import Document, Generation, LLMResult\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "class LocalOllamaLLM(BaseLLM):\n",
    "    api_url : str\n",
    "    def _generate(self, prompt, stop):\n",
    "        response = requests.post(f\"{self.api_url}/api/generate\", json={\"model\": \"llama3.1\", \"prompt\": str(prompt) })\n",
    "        response.raise_for_status()\n",
    "        response_text=''.join([json.loads(line)['response'] for line in response.text.splitlines()])\n",
    "        generations=[]\n",
    "        generations.append([Generation(text=response_text)])\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "\n",
    "    def _llm_type(self):\n",
    "        return \"local\"  # Or whatever type is appropriate for your local setup\n",
    "\n",
    "llm = LocalOllamaLLM(api_url=\"http://127.0.0.1:11434\")\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "vector_store = Chroma(embedding_function=embedder, persist_directory=\"./chroma_db\")\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \" Répondez à la question posée \"\n",
    "    \" Utilisez le contexte (sélection des meilleurs paragraphes liés à la question) donné pour répondre à la question \"\n",
    "    \" Si la réponse ne se trouve pas dans le contexte, répondez par 'Je ne sais pas'\"\n",
    "    \" Contexte : {context}  \"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "def search_and_invoke_llm(vector_store,index,query,k=5):\n",
    "    if k==0:\n",
    "        print(f\"bug with {index}\")\n",
    "        return None\n",
    "    else:\n",
    "        pass\n",
    "    try:\n",
    "        retriever=vector_store.as_retriever(\n",
    "        search_kwargs={\n",
    "                \"k\": k, \n",
    "                \"filter\": {'index': index}\n",
    "            }\n",
    "        )\n",
    "        chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "        result=chain.invoke({\"input\": query})\n",
    "        return result\n",
    "    except:\n",
    "        search_and_invoke_llm(vector_store,index,query,k=k-1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c603d-1a06-4cac-9133-50671084d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS=[\"Quelles sont les règles de majoration des heures supplémentaires ?\",\n",
    "          \"De combien est le contingent annuel d'heures supplémentaires ?\",\n",
    "          \"Quelle est la durée maximale hebdomadaire de travail ?\",\n",
    "          \"Quelles les contreparties en repos ?\",\n",
    "          \"De combien est le repos compensateur obligatoire?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a676a-ae0c-4952-bca8-d542bab44f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"./10p_accords_publics_et_thematiques_240815_sub_heures_supp.parquet\"\n",
    "df=pd.read_parquet(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5106c-409a-45a8-bd06-43fb865a0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_df=[]\n",
    "Path(\"results\").mkdir(parents=True, exist_ok=True)\n",
    "for index, row in df.iterrows():\n",
    "    dict_answer=dict()\n",
    "    answer=\"\"\n",
    "    for Q0 in QUESTIONS:\n",
    "        if ans:=search_and_invoke_llm(vector_store,index,Q0,k=2):\n",
    "            answer_txt=ans['answer']\n",
    "            answer+=f\"{ans['input']} {answer_txt} \\n\"\n",
    "    with open(f\"results/{index}.answer\",\"w\") as file:\n",
    "        file.write(answer)\n",
    "    \n",
    "    with open(f\"results/{index}.context\",\"w\") as file:\n",
    "        try:\n",
    "            file.write(\"\\n-----\\n\".join(list(map(lambda x : x.page_content,ans['context']))))\n",
    "        except:\n",
    "            file.write(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
