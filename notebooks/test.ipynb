{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a74e4-bdff-4c0c-adec-58d7f643bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, OllamaEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document, Generation, LLMResult\n",
    "from langchain.llms import Ollama, BaseLLM\n",
    "from langchain.chains import StuffDocumentsChain, RetrievalQA, LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import OpenAI\n",
    "import json\n",
    "import requests\n",
    "\n",
    "class LocalOllamaLLM(BaseLLM):\n",
    "    api_url : str\n",
    "    def _generate(self, prompt, stop):\n",
    "        print(f\"_generate : {prompt} ; {type(prompt)}\")\n",
    "        response = requests.post(f\"{self.api_url}/api/generate\", json={\"model\": \"mistral-large\", \"prompt\": str(prompt) })\n",
    "        response.raise_for_status()\n",
    "        response_text=''.join([json.loads(line)['response'] for line in response.text.splitlines()])\n",
    "        generations=[]\n",
    "        generations.append([Generation(text=response_text)])\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "\n",
    "    def _llm_type(self):\n",
    "        return \"local\"  # Or whatever type is appropriate for your local setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0195e-64f5-4f49-ae80-226f73f5ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "vector_store = Chroma(embedding_function=embedder)\n",
    "\n",
    "# Create some sample documents\n",
    "documents = [\n",
    "    Document(page_content=\"This is the first document.\", metadata={\"label\": \"doc1\"}),\n",
    "    Document(page_content=\"This is the second document.\", metadata={\"label\": \"doc2\"}),\n",
    "    Document(page_content=\"This is the third document.\", metadata={\"label\": \"doc3\"}),\n",
    "    Document(page_content=\"The Sun is the star at the center of the Solar System. It is a massive, nearly perfect sphere of hot plasma, heated to incandescence by nuclear fusion reactions in its core, radiating the energy from its surface mainly as visible light and infrared radiation with 10% at ultraviolet energies. It is by far the most important source of energy for life on Earth. The Sun has been an object of veneration in many cultures. It has been a central subject for astronomical research since antiquity.\", metadata={\"label\": \"doc4\"}),\n",
    "             \n",
    "]\n",
    "\n",
    "# Add documents to the vector store\n",
    "vector_store.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f30fa-2707-479b-a3fd-b89a11d10a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LocalOllamaLLM(api_url=\"http://127.0.0.1:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2462b-da13-4eb5-815b-a01802c7b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever= vector_store.as_retriever()\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "query= \"what is the sun?\"\n",
    "chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600586e-8933-4e6b-8d7c-e3f46ba304ad",
   "metadata": {},
   "source": [
    "## Simple example with requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b46065-35f8-441f-a286-a6ad539699b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# URL to which the request is to be sent\n",
    "url = 'http://localhost:11434/api/generate'\n",
    "\n",
    "# Data to be sent in the POST request\n",
    "data = {\n",
    "    \"model\": \"mistral-large\",\n",
    "    \"prompt\": \"\"\"\n",
    "    [\"System: Use the given context to answer the question. If you don't know the answer, say you don't know. Use three sentence maximum and keep the answer concise. Context: This is the first document.\\n\\nThis is the second document.\\n\\nThis is the third document.\\nHuman: what is the sun?\"]\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Sending the POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Printing the response\n",
    "print(response.status_code)  # HTTP status code\n",
    "print(response.text)\n",
    "complete_response = ''.join([json.loads(line)['response'] for line in response.text.splitlines()])\n",
    "\n",
    "# Print the complete response\n",
    "print(complete_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd1c49-952f-4f5c-949d-302acf079d41",
   "metadata": {},
   "source": [
    "## EXPERIMENTAL"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b123e9c7-210f-4ed9-a8f6-27dc716bc07b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "    template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize this content: {context}\"\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86663043-8ad3-4037-8e8e-4d16787cd7bb",
   "metadata": {},
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "retriever= vector_store.as_retriever()\n",
    "rag_chain = ( \n",
    " {\"context\": retriever, \"page_content\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d26bbf7-6ae4-46b0-8c3e-c4f2a4549b0a",
   "metadata": {},
   "source": [
    "rag_chain.invoke(\"Combien de documents ?\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49aa08ac-7132-4879-9c55-97a02fdc4897",
   "metadata": {},
   "source": [
    "qa_chain = RetrievalQA(\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    combine_documents_chain=chain\n",
    ")\n",
    "# Perform a query\n",
    "query = \"Find documents similar to this query.\"\n",
    "results = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the search results\n",
    "print(\"Search Results:\")\n",
    "for result in results[\"results\"]:\n",
    "    print(f\"Content: {result.page_content}, Metadata: {result.metadata}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2ed889d-3f16-4abc-842f-75a3b409e8a1",
   "metadata": {},
   "source": [
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA(\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    combine_documents_chain=chain\n",
    ")\n",
    "\n",
    "# Example query to the QA chain\n",
    "query = \"What is the main idea of the documents?\"\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
