{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a74e4-bdff-4c0c-adec-58d7f643bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, OllamaEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.llms import Ollama, BaseLLM\n",
    "from langchain.chains import StuffDocumentsChain, RetrievalQA\n",
    "import requests\n",
    "\n",
    "class LocalOllamaLLM(BaseLLM):\n",
    "    api_url : str\n",
    "    def _generate(self, prompt):\n",
    "        response = requests.post(self.api_url, json={\"prompt\": prompt})\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"text\"]\n",
    "\n",
    "    def _llm_type(self):\n",
    "        return \"local\"  # Or whatever type is appropriate for your local setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d684bdd2-84d8-42a0-b780-b34e43ef4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f0195e-64f5-4f49-ae80-226f73f5ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(embedding_function=embedder)\n",
    "\n",
    "# Create some sample documents\n",
    "documents = [\n",
    "    Document(page_content=\"This is the first document.\", metadata={\"label\": \"doc1\"}),\n",
    "    Document(page_content=\"This is the second document.\", metadata={\"label\": \"doc2\"}),\n",
    "    Document(page_content=\"This is the third document.\", metadata={\"label\": \"doc3\"}),\n",
    "]\n",
    "\n",
    "# Add documents to the vector store\n",
    "vector_store.add_documents(documents)\n",
    "\n",
    "# Perform a similarity search\n",
    "query = \"Find documents similar to this query : third.\"\n",
    "results = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "# Print the search results\n",
    "print(\"Search Results:\")\n",
    "for result in results:\n",
    "    print(f\"Content: {result.page_content}, Metadata: {result.metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f30fa-2707-479b-a3fd-b89a11d10a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LocalOllamaLLM(api_url=\"http://127.0.0.1:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c74c0-96eb-4596-838c-15746f53d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_docs_chain = StuffDocumentsChain()\n",
    "\n",
    "# Create a RetrievalQA chain with the combine_docs_chain\n",
    "qa_chain = RetrievalQA(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    combine_documents_chain=combine_docs_chain\n",
    ")\n",
    "# Perform a query\n",
    "query = \"Find documents similar to this query.\"\n",
    "results = qa_chain({\"query\": query})\n",
    "\n",
    "# Print the search results\n",
    "print(\"Search Results:\")\n",
    "for result in results[\"results\"]:\n",
    "    print(f\"Content: {result.page_content}, Metadata: {result.metadata}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.env)",
   "language": "python",
   "name": "dotenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
