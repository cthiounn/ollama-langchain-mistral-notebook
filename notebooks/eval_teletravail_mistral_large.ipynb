{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f5525b-6bac-404d-9724-96de51d76185",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp s3/$VAULT_TOP_DIR/Accords/Construction_dataset_public/Dataset_public_accords_teletravail_Dares.parquet ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e5d88-a743-4eed-8aa6-5806e6461f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, OllamaEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document, Generation, LLMResult\n",
    "from langchain.llms import Ollama, BaseLLM\n",
    "from langchain.chains import StuffDocumentsChain, RetrievalQA, LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "\n",
    "class LocalOllamaLLM(BaseLLM):\n",
    "    api_url : str\n",
    "    def _generate(self, prompt, stop):\n",
    "        response = requests.post(f\"{self.api_url}/api/generate\", json={\"model\": \"mistral-large\", \"prompt\": str(prompt) })\n",
    "        response.raise_for_status()\n",
    "        response_text=''.join([json.loads(line)['response'] for line in response.text.splitlines()])\n",
    "        generations=[]\n",
    "        generations.append([Generation(text=response_text)])\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "\n",
    "    def _llm_type(self):\n",
    "        return \"local\"  # Or whatever type is appropriate for your local setup\n",
    "\n",
    "llm = LocalOllamaLLM(api_url=\"http://127.0.0.1:11434\")\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "system_prompt = (\n",
    "    \" Utilisez le contexte donné pour répondre à la question.  \"\n",
    "    \" Si vous ne connaissez pas la réponse, dites que vous ne savez pas.  \"\n",
    "    \" Utilisez trois phrases au maximum et soyez concis dans votre réponse. \"\n",
    "    \" En premier lieu, répondre en donnant une variable : variable=(valeur ou None)  . \"\n",
    "    \" S'il y a plusieurs valeurs possibles, prendre le max : variable=max(valeurs ou None)  . \"\n",
    "    \" Contexte : {context}  \"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c71950-76d0-42f9-8e27-45840083a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"Dataset_public_accords_teletravail_Dares.parquet\"\n",
    "df=pd.read_parquet(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9eae4c-bdbc-4437-9583-fcd8cd77d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir = Path('results').mkdir(exist_ok=True)\n",
    "for index, row in df.iterrows():\n",
    "    text = df.texte_complet_accord[index]\n",
    "    texts = text_splitter.create_documents([text])\n",
    "    vector_store = Chroma(embedding_function=embedder)\n",
    "    vector_store.add_documents(texts)\n",
    "    retriever= vector_store.as_retriever()\n",
    "    chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    query= \"Combien de jour de télétravail par semaine est autorisé au maximum ?\"\n",
    "    result=chain.invoke({\"input\": query})\n",
    "    with open(f\"results/{index}.answer\",\"w\") as f:\n",
    "        f.write(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
