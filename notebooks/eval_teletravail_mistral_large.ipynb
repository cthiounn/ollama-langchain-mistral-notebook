{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f5525b-6bac-404d-9724-96de51d76185",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp s3/$VAULT_TOP_DIR/Accords/Construction_dataset_public/Dataset_public_accords_teletravail_Dares.parquet ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c8221-e6eb-4392-99a1-70d48a680d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp -r s3/$VAULT_TOP_DIR/Accords/chroma_db ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347945a-d2c9-4248-8ea5-e84b15b4ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp -r s3/$VAULT_TOP_DIR/Accords/Large2/results ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e5d88-a743-4eed-8aa6-5806e6461f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, OllamaEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema import Document, Generation, LLMResult\n",
    "from langchain.llms import Ollama, BaseLLM\n",
    "from langchain.chains import StuffDocumentsChain, RetrievalQA, LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "\n",
    "#\"seed\":1337,\"temperature\":1,\"num_ctx\":100,\n",
    "class LocalOllamaLLM(BaseLLM):\n",
    "    api_url : str\n",
    "    def _generate(self, prompt, stop):\n",
    "        print(str(prompt))\n",
    "        response = requests.post(f\"{self.api_url}/api/generate\", json={\"model\": \"mistral-large\", \"prompt\": str(prompt),\"options\":{\"stop\":[\",\",\".\"]} })\n",
    "        response.raise_for_status()\n",
    "        response_text=''.join([json.loads(line)['response'] for line in response.text.splitlines()])\n",
    "        generations=[]\n",
    "        generations.append([Generation(text=response_text)])\n",
    "        return LLMResult(generations=generations)\n",
    "\n",
    "\n",
    "    def _llm_type(self):\n",
    "        return \"local\"  \n",
    "        \n",
    "llm = LocalOllamaLLM(api_url=\"http://127.0.0.1:11434\")\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "system_prompt = (\n",
    "    \" Répondez à la question posée \"\n",
    "    \" Utilisez le contexte (sélection des meilleurs paragraphes liés à la question) donné pour répondre à la question\"\n",
    "    \" Pour une question Oui ou Non, répondez Oui ou Non \"\n",
    "    #\" Si la réponse ne se trouve pas dans le contexte, répondez par 'Non'\"\n",
    "    \" Contexte : {context}  \"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c71950-76d0-42f9-8e27-45840083a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"Dataset_public_accords_teletravail_Dares.parquet\"\n",
    "df=pd.read_parquet(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370241b6-4a06-4474-87ef-889efcf7b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(embedding_function=embedder,persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18be6ed-2b47-4472-8746-8c427c8532be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_invoke_llm(vector_store,index,query,k=5):\n",
    "    if k==0:\n",
    "        print(f\"bug with {index}\")\n",
    "        return None\n",
    "    elif k==1:\n",
    "        pass\n",
    "        #print(f\" trying k=1 : {index}\")\n",
    "    else:\n",
    "        pass\n",
    "    try:\n",
    "        retriever=vector_store.as_retriever(\n",
    "        search_kwargs={\n",
    "                \"k\": k, \n",
    "                \"filter\": {'index': index}\n",
    "            }\n",
    "        )\n",
    "        chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "        result=chain.invoke({\"input\": query})\n",
    "        return result\n",
    "    except:\n",
    "        search_and_invoke_llm(vector_store,index,query,k=k-1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03b4133-fbe1-49e5-a493-7f8075788b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q0=\"Oui ou non : est-ce que le contexte mentionne le télétravail ?\"\n",
    "\n",
    "Q1=\"Oui ou non : est-ce qu'un nombre de jour de télétravail est mentionné ?\"\n",
    "Q2=\"Oui ou non : est-ce qu'une limite de jour de télétravail est mentionné ?\"\n",
    "Q3=\"Oui ou non : est-ce qu'une journée de télétravail est mentionné ?\"\n",
    "Q4=\"Oui ou non : est-ce qu'un nombre de jour de télétravail peut être déduit ?\"\n",
    "Q5=\"Oui ou non : est-ce qu'une limite de jour de télétravail peut être déduit ?\"\n",
    "\n",
    "LIST_OF_QUESTIONS=[Q1,Q2,Q3,Q4,Q5]\n",
    "LIST_OF_QUESTIONS=[Q0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9eae4c-bdbc-4437-9583-fcd8cd77d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "already_done={el.split(\"/\")[1].split(\".\")[0] for el in glob(\"results/*.answer\")}\n",
    "new_dir = Path('results').mkdir(exist_ok=True)\n",
    "#query= f\"{Q1} {Q2} {Q3} {Q4} {Q5}\"\n",
    "for index, row in df.iterrows():\n",
    "    answer=\"\"\n",
    "    if index not in already_done:\n",
    "\n",
    "        for Q in LIST_OF_QUESTIONS:\n",
    "            if ans:=search_and_invoke_llm(vector_store,index,Q,k=1):\n",
    "                answer_txt=ans['answer']\n",
    "                answer += answer_txt\n",
    "            answer += \"\\n-----\\n\"\n",
    "        #query= \"Combien de jour de télétravail par semaine est autorisé au maximum ?\"\n",
    "        #answer += search_and_invoke_llm(vector_store,index,query)\n",
    "        print(index,df.nombre_jour_hebdo_TT_annotee[index], answer)\n",
    "        if answer:\n",
    "            with open(f\"results/{index}.answer\",\"w\") as f:\n",
    "                f.write(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61387dca-3d00-40c2-9aeb-b00ba670f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_store.similarity_search(Q4,2,{'index': \"T59L21013979\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47593af5-4d43-4129-8e85-6cb8245a725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mc cp -r results s3/$VAULT_TOP_DIR/Accords/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb18d9-b60b-45b6-a731-46bcc3953beb",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca963eb5-1a06-45de-88f8-5d77442c1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_in_chunks(lst, chunk_size=5):\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3789c34-84af-47ef-a163-95a39bb5b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt2 = (\n",
    "    \" Utilisez le contexte donné pour répondre à la question.  \"\n",
    "    \" Si vous ne connaissez pas la réponse, dites que vous ne savez pas.  \"\n",
    "    \" Utilisez trois phrases au maximum et soyez concis dans votre réponse. \"\n",
    "    \" En premier lieu, répondre en donnant une variable : variable=(valeur ou None)  . \"\n",
    "    \" S'il y a plusieurs valeurs possibles, prendre le max : variable=max(valeurs ou None)  . \"\n",
    "    \" Contexte : {context}  \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471afe99-3aa7-4b52-b7c7-0b6104ffcf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \" Utilisez le contexte (sélection des meilleurs paragraphes liés à la question) donné pour répondre à la question en un mot\"\n",
    "    \" Répondez seulement par un unique mot français\"\n",
    "    \" Si la réponse ne se trouve pas dans le contexte, répondez par 'Non'\"\n",
    "#    \" Commencez toutes vos réponses par 'Oui' ou par 'Non', puis arrêtez votre réponse\"\n",
    "#    \" Si la réponse commence par 'Oui', répondez seulement par 'Oui'\"\n",
    "#    \" Si la réponse commence par 'Non', répondez seulement par 'Non'\"\n",
    "    \" Contexte : {context}  \"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
